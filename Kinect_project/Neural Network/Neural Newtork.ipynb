{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PmmXFM2XHhg"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2oglIi4bXHhh"
   },
   "source": [
    "We'll now use a Neural Network to predict the players identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnMLcfIjXHhi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EV9DklKXHhn"
   },
   "source": [
    "## We'll start with the features and the topology proposed by the last year and train the NN with it. After we'll try with our features (generated by wave instead of each balloon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtFs1mzRXHho"
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('./features/kate_data_julien_sarah.csv', delimiter=',')\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAx-oI9ZXHhr"
   },
   "outputs": [],
   "source": [
    "training_ratio = 0.85\n",
    "l = len(data)\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X_train = X[:int(l*training_ratio)]\n",
    "X_test = X[int(l*training_ratio):]\n",
    "y_train = y[:int(l*training_ratio)]/2\n",
    "y_test = y[int(l*training_ratio):]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d61VhYOuXHhx"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype(int))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzJ5_3VTXHh2"
   },
   "source": [
    "# Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNIXJgglXHh4",
    "outputId": "a8fd3469-9505-452c-ed5e-15cceaa508e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reconstruction error with 2 PCs: 7.471\n",
      "2234.6621633472378\n",
      "-22.639794956373947\n",
      "128.40015756257122\n",
      "-369.87814850662505\n",
      "94\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "mu = X_train.mean(axis=0)\n",
    "U,s,V = np.linalg.svd(X_train - mu, full_matrices=False)\n",
    "Zpca = np.dot(X_train - mu, V.transpose())\n",
    "\n",
    "Rpca = np.dot(Zpca[:,:2], V[:2,:]) + mu    # reconstruction\n",
    "err = np.sum((X_train-Rpca)**2)/Rpca.shape[0]/Rpca.shape[1]\n",
    "print('PCA reconstruction error with 2 PCs: ' + str(round(err,3)));\n",
    "print(max(Zpca[:,0]))\n",
    "print(min(Zpca[:,0]))\n",
    "print(max(Zpca[:,1]))\n",
    "print(min(Zpca[:,1]))\n",
    "\n",
    "print(np.argmax(Zpca[:,0]))\n",
    "print(np.argmax(Zpca[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbNmZqTlXHh_"
   },
   "source": [
    "# Building and training of a dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnzuXQMBXHh_",
    "outputId": "ba6d8cfa-1477-4606-b2fc-a92d603cfe3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/20\n",
      "1360/1360 [==============================] - 1s 605us/step - loss: 0.6897 - acc: 0.5272 - val_loss: 0.6454 - val_acc: 0.4250\n",
      "Epoch 2/20\n",
      "1360/1360 [==============================] - 0s 168us/step - loss: 0.5857 - acc: 0.6199 - val_loss: 0.5939 - val_acc: 0.5500\n",
      "Epoch 3/20\n",
      "1360/1360 [==============================] - 0s 173us/step - loss: 0.5346 - acc: 0.7515 - val_loss: 0.5569 - val_acc: 0.7458\n",
      "Epoch 4/20\n",
      "1360/1360 [==============================] - 0s 165us/step - loss: 0.5692 - acc: 0.7603 - val_loss: 0.5654 - val_acc: 0.5750\n",
      "Epoch 5/20\n",
      "1360/1360 [==============================] - 0s 216us/step - loss: 0.5196 - acc: 0.7831 - val_loss: 0.5552 - val_acc: 0.6583\n",
      "Epoch 6/20\n",
      "1360/1360 [==============================] - 0s 169us/step - loss: 0.5210 - acc: 0.7596 - val_loss: 0.5590 - val_acc: 0.5958\n",
      "Epoch 7/20\n",
      "1360/1360 [==============================] - 0s 171us/step - loss: 0.4939 - acc: 0.7816 - val_loss: 0.6841 - val_acc: 0.7333\n",
      "Epoch 8/20\n",
      "1360/1360 [==============================] - 0s 170us/step - loss: 0.5169 - acc: 0.7632 - val_loss: 0.5652 - val_acc: 0.7917\n",
      "Epoch 9/20\n",
      "1360/1360 [==============================] - 0s 169us/step - loss: 0.4880 - acc: 0.8125 - val_loss: 0.6119 - val_acc: 0.7667\n",
      "Epoch 10/20\n",
      "1360/1360 [==============================] - 0s 170us/step - loss: 0.4857 - acc: 0.8213 - val_loss: 0.5928 - val_acc: 0.8125\n",
      "Epoch 11/20\n",
      "1360/1360 [==============================] - 0s 172us/step - loss: 0.4859 - acc: 0.8287 - val_loss: 0.4065 - val_acc: 0.8250\n",
      "Epoch 12/20\n",
      "1360/1360 [==============================] - 0s 177us/step - loss: 0.3807 - acc: 0.8169 - val_loss: 0.4039 - val_acc: 0.8250\n",
      "Epoch 13/20\n",
      "1360/1360 [==============================] - 0s 172us/step - loss: 0.3182 - acc: 0.8449 - val_loss: 0.4356 - val_acc: 0.8125\n",
      "Epoch 14/20\n",
      "1360/1360 [==============================] - 0s 171us/step - loss: 0.3211 - acc: 0.8441 - val_loss: 0.3897 - val_acc: 0.8250\n",
      "Epoch 15/20\n",
      "1360/1360 [==============================] - 0s 170us/step - loss: 0.2933 - acc: 0.8581 - val_loss: 0.3797 - val_acc: 0.8333\n",
      "Epoch 16/20\n",
      "1360/1360 [==============================] - 0s 197us/step - loss: 0.2959 - acc: 0.8566 - val_loss: 0.4519 - val_acc: 0.8208\n",
      "Epoch 17/20\n",
      "1360/1360 [==============================] - 0s 174us/step - loss: 0.2719 - acc: 0.8654 - val_loss: 0.3671 - val_acc: 0.8250\n",
      "Epoch 18/20\n",
      "1360/1360 [==============================] - 0s 169us/step - loss: 0.2671 - acc: 0.8632 - val_loss: 0.3211 - val_acc: 0.8250\n",
      "Epoch 19/20\n",
      "1360/1360 [==============================] - 0s 176us/step - loss: 0.2833 - acc: 0.8632 - val_loss: 0.4545 - val_acc: 0.8208\n",
      "Epoch 20/20\n",
      "1360/1360 [==============================] - 0s 179us/step - loss: 0.2736 - acc: 0.8647 - val_loss: 0.4859 - val_acc: 0.8083\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(150,  activation='relu', input_shape=(105,)))\n",
    "#m.add(Dense(150,  activation='relu')) \n",
    "m.add(Dense(150,  activation='relu'))\n",
    "m.add(Dense(150,  activation='relu'))\n",
    "m.add(Dense(50,  activation='relu'))\n",
    "m.add(Dense(2,  activation='sigmoid'))\n",
    "m.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = m.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMdEB1fYXHiE"
   },
   "outputs": [],
   "source": [
    "y_pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1UFUpo4EXHiF",
    "outputId": "0bc6d61f-6ee8-4b38-fa5f-8b2d55d51d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 37us/step\n",
      "Précision old features: 0.81\n"
     ]
    }
   ],
   "source": [
    "accuracy = m.evaluate(X_test, y_test)[1]\n",
    "print(\"Précision old features: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cPsbLAeXHiI"
   },
   "source": [
    "## Now let's try with our features and with a different topology. Since we have computed a 12 dimension feature-vector, it would make no sense to use layers with more than 100 neurons as the last year group did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uzDjwuJoXHiJ"
   },
   "outputs": [],
   "source": [
    "X = np.genfromtxt('./features/features_wave_julian_sarah.csv', delimiter=',')\n",
    "y = np.genfromtxt('./features/output_wave_julian_sarah.csv', delimiter=',')\n",
    "\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "training_ratio = 0.85\n",
    "l = len(y)\n",
    "X_train = X[:int(l*training_ratio)]\n",
    "X_test = X[int(l*training_ratio):]\n",
    "y_train = y[:int(l*training_ratio)]/2\n",
    "y_test = y[int(l*training_ratio):]/2\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype(int))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCfabp_VXHiL"
   },
   "source": [
    "In our case we have 12 dimensions to features instead. Let's apply the NN directly without a PCA to compare later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lf5irXwXXHiL",
    "outputId": "6a92ad0c-bbda-49da-ea70-67c9205a1277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/20\n",
      "1360/1360 [==============================] - 1s 466us/step - loss: 0.5555 - acc: 0.7846 - val_loss: 0.5473 - val_acc: 0.8167\n",
      "Epoch 2/20\n",
      "1360/1360 [==============================] - 0s 86us/step - loss: 0.5331 - acc: 0.7926 - val_loss: 0.5314 - val_acc: 0.7958\n",
      "Epoch 3/20\n",
      "1360/1360 [==============================] - 0s 96us/step - loss: 0.5195 - acc: 0.7993 - val_loss: 0.5249 - val_acc: 0.8000\n",
      "Epoch 4/20\n",
      "1360/1360 [==============================] - 0s 85us/step - loss: 0.5105 - acc: 0.8037 - val_loss: 0.5086 - val_acc: 0.8167\n",
      "Epoch 5/20\n",
      "1360/1360 [==============================] - 0s 84us/step - loss: 0.4987 - acc: 0.8000 - val_loss: 0.4935 - val_acc: 0.8208\n",
      "Epoch 6/20\n",
      "1360/1360 [==============================] - 0s 81us/step - loss: 0.4950 - acc: 0.8000 - val_loss: 0.4858 - val_acc: 0.8208\n",
      "Epoch 7/20\n",
      "1360/1360 [==============================] - 0s 87us/step - loss: 0.4772 - acc: 0.8074 - val_loss: 0.4777 - val_acc: 0.8333\n",
      "Epoch 8/20\n",
      "1360/1360 [==============================] - 0s 81us/step - loss: 0.4680 - acc: 0.8096 - val_loss: 0.4604 - val_acc: 0.8375\n",
      "Epoch 9/20\n",
      "1360/1360 [==============================] - 0s 84us/step - loss: 0.4561 - acc: 0.8096 - val_loss: 0.4660 - val_acc: 0.8292\n",
      "Epoch 10/20\n",
      "1360/1360 [==============================] - 0s 84us/step - loss: 0.4521 - acc: 0.8103 - val_loss: 0.4357 - val_acc: 0.7917\n",
      "Epoch 11/20\n",
      "1360/1360 [==============================] - 0s 84us/step - loss: 0.4362 - acc: 0.8162 - val_loss: 0.4454 - val_acc: 0.7917\n",
      "Epoch 12/20\n",
      "1360/1360 [==============================] - 0s 96us/step - loss: 0.4366 - acc: 0.8132 - val_loss: 0.4187 - val_acc: 0.8500\n",
      "Epoch 13/20\n",
      "1360/1360 [==============================] - 0s 84us/step - loss: 0.4244 - acc: 0.8147 - val_loss: 0.4045 - val_acc: 0.8542\n",
      "Epoch 14/20\n",
      "1360/1360 [==============================] - 0s 87us/step - loss: 0.4197 - acc: 0.8257 - val_loss: 0.4476 - val_acc: 0.8167\n",
      "Epoch 15/20\n",
      "1360/1360 [==============================] - 0s 104us/step - loss: 0.4131 - acc: 0.8221 - val_loss: 0.3885 - val_acc: 0.8500\n",
      "Epoch 16/20\n",
      "1360/1360 [==============================] - 0s 91us/step - loss: 0.4045 - acc: 0.8243 - val_loss: 0.3961 - val_acc: 0.8125\n",
      "Epoch 17/20\n",
      "1360/1360 [==============================] - 0s 92us/step - loss: 0.3923 - acc: 0.8324 - val_loss: 0.3728 - val_acc: 0.8667\n",
      "Epoch 18/20\n",
      "1360/1360 [==============================] - 0s 89us/step - loss: 0.3942 - acc: 0.8228 - val_loss: 0.3783 - val_acc: 0.8542\n",
      "Epoch 19/20\n",
      "1360/1360 [==============================] - 0s 84us/step - loss: 0.3831 - acc: 0.8368 - val_loss: 0.3909 - val_acc: 0.8542\n",
      "Epoch 20/20\n",
      "1360/1360 [==============================] - 0s 93us/step - loss: 0.3746 - acc: 0.8360 - val_loss: 0.3679 - val_acc: 0.8750\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(15,  activation='relu', input_shape=(12,)))\n",
    "m.add(Dense(15,  activation='relu'))\n",
    "m.add(Dense(15,  activation='relu'))\n",
    "m.add(Dense(4,  activation='relu'))\n",
    "m.add(Dense(2,  activation='sigmoid'))\n",
    "m.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = m.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_aqe1jDXHiO",
    "outputId": "e434e833-c058-4d82-a06a-439ceb9a491c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 29us/step\n",
      "Précision New features: 0.88\n"
     ]
    }
   ],
   "source": [
    "accuracy = m.evaluate(X_test, y_test)[1]\n",
    "print(\"Précision New features: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDBzWC_7XHiR"
   },
   "source": [
    "# We got a precision of 88%, which is better than the 81% of the last year. But we would need to do an average to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ki1TL03NXHiS"
   },
   "source": [
    "Let's apply PCA now to reduce the dimension to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnPODEJFXHiT"
   },
   "outputs": [],
   "source": [
    "model_pca3 = PCA(n_components=3)\n",
    "\n",
    "# On entraîne notre modèle (fit) sur les données\n",
    "model_pca3.fit(X)\n",
    "\n",
    "# On applique le résultat sur nos données :\n",
    "X_reduced3 = model_pca3.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBZoIkzNXHiW"
   },
   "outputs": [],
   "source": [
    "training_ratio = 0.85\n",
    "l = len(y)\n",
    "X_train = X_reduced3[:int(l*training_ratio)]\n",
    "X_test = X_reduced3[int(l*training_ratio):]\n",
    "y_train = y[:int(l*training_ratio)]/2\n",
    "y_test = y[int(l*training_ratio):]/2\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype(int))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nA8fTguzXHia",
    "outputId": "6a779955-036b-4229-e08e-6a8f02c60850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/20\n",
      "1360/1360 [==============================] - 1s 555us/step - loss: 0.5776 - acc: 0.7301 - val_loss: 0.5513 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      "1360/1360 [==============================] - 0s 91us/step - loss: 0.4966 - acc: 0.7831 - val_loss: 0.5360 - val_acc: 0.7583\n",
      "Epoch 3/20\n",
      "1360/1360 [==============================] - 0s 87us/step - loss: 0.4722 - acc: 0.7860 - val_loss: 0.5485 - val_acc: 0.7333\n",
      "Epoch 4/20\n",
      "1360/1360 [==============================] - 0s 88us/step - loss: 0.4613 - acc: 0.7897 - val_loss: 0.5361 - val_acc: 0.7583\n",
      "Epoch 5/20\n",
      "1360/1360 [==============================] - 0s 86us/step - loss: 0.4488 - acc: 0.7949 - val_loss: 0.5459 - val_acc: 0.7458\n",
      "Epoch 6/20\n",
      "1360/1360 [==============================] - 0s 87us/step - loss: 0.4395 - acc: 0.7963 - val_loss: 0.5291 - val_acc: 0.7375\n",
      "Epoch 7/20\n",
      "1360/1360 [==============================] - 0s 102us/step - loss: 0.4388 - acc: 0.7934 - val_loss: 0.5254 - val_acc: 0.7417\n",
      "Epoch 8/20\n",
      "1360/1360 [==============================] - 0s 97us/step - loss: 0.4293 - acc: 0.7919 - val_loss: 0.5402 - val_acc: 0.7292\n",
      "Epoch 9/20\n",
      "1360/1360 [==============================] - 0s 98us/step - loss: 0.4341 - acc: 0.7963 - val_loss: 0.5272 - val_acc: 0.7375\n",
      "Epoch 10/20\n",
      "1360/1360 [==============================] - 0s 95us/step - loss: 0.4172 - acc: 0.8029 - val_loss: 0.5676 - val_acc: 0.7333\n",
      "Epoch 11/20\n",
      "1360/1360 [==============================] - 0s 90us/step - loss: 0.4199 - acc: 0.8022 - val_loss: 0.5253 - val_acc: 0.7458\n",
      "Epoch 12/20\n",
      "1360/1360 [==============================] - 0s 95us/step - loss: 0.4155 - acc: 0.8000 - val_loss: 0.5548 - val_acc: 0.7250\n",
      "Epoch 13/20\n",
      "1360/1360 [==============================] - 0s 90us/step - loss: 0.4158 - acc: 0.7963 - val_loss: 0.5477 - val_acc: 0.7375\n",
      "Epoch 14/20\n",
      "1360/1360 [==============================] - 0s 107us/step - loss: 0.4137 - acc: 0.8007 - val_loss: 0.5076 - val_acc: 0.7500\n",
      "Epoch 15/20\n",
      "1360/1360 [==============================] - 0s 101us/step - loss: 0.4128 - acc: 0.8007 - val_loss: 0.5281 - val_acc: 0.7458\n",
      "Epoch 16/20\n",
      "1360/1360 [==============================] - 0s 89us/step - loss: 0.4045 - acc: 0.8088 - val_loss: 0.5103 - val_acc: 0.7500\n",
      "Epoch 17/20\n",
      "1360/1360 [==============================] - 0s 88us/step - loss: 0.4058 - acc: 0.7978 - val_loss: 0.5145 - val_acc: 0.7417\n",
      "Epoch 18/20\n",
      "1360/1360 [==============================] - 0s 91us/step - loss: 0.4036 - acc: 0.8110 - val_loss: 0.5468 - val_acc: 0.7583\n",
      "Epoch 19/20\n",
      "1360/1360 [==============================] - 0s 89us/step - loss: 0.4095 - acc: 0.8037 - val_loss: 0.5118 - val_acc: 0.7583\n",
      "Epoch 20/20\n",
      "1360/1360 [==============================] - 0s 87us/step - loss: 0.4061 - acc: 0.8110 - val_loss: 0.5160 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(20,  activation='relu', input_shape=(3,)))\n",
    "#m.add(Dense(20,  activation='relu')) \n",
    "m.add(Dense(20,  activation='relu'))\n",
    "m.add(Dense(20,  activation='relu'))\n",
    "m.add(Dense(5,  activation='relu'))\n",
    "m.add(Dense(2,  activation='sigmoid'))\n",
    "m.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = m.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E98cFZP5XHid"
   },
   "source": [
    "Using a PCA doesn't seems to improve the accuracy, but the accuracy seems to be more stable with respect to the epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UrwqWQ2XHid"
   },
   "source": [
    "## We conclude that the Neural Network gives a better precision that our previous alorithms. Computing the features by wave instead of by balloons also seems to improve the Neural Network precision. The only disavantage is that we get less data by doing so. If few data is available, it might be better to use the features computed by balloon instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Neural Newtork.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
