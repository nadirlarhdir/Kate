{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use a Neural Network to predict the players identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the feature data generated by the last year and train the NN with it. After we'll try our new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('kate_data_julien_sarah.csv', delimiter=',')\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.85\n",
    "l = len(data)\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "X_train = X[:int(l*training_ratio)]\n",
    "X_test = X[int(l*training_ratio):]\n",
    "y_train = y[:int(l*training_ratio)]/2\n",
    "y_test = y[int(l*training_ratio):]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype(int))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't understand this part... Just copied. I think they've manually applied the dimension reduction and created this \"error\". If so, we could do that directly using SciKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA reconstruction error with 2 PCs: 6.255\n",
      "2233.586102314126\n",
      "-22.32930875280139\n",
      "159.2231182347611\n",
      "-105.72923985409766\n",
      "966\n",
      "966\n"
     ]
    }
   ],
   "source": [
    "mu = X_train.mean(axis=0)\n",
    "U,s,V = np.linalg.svd(X_train - mu, full_matrices=False)\n",
    "Zpca = np.dot(X_train - mu, V.transpose())\n",
    "\n",
    "Rpca = np.dot(Zpca[:,:2], V[:2,:]) + mu    # reconstruction\n",
    "err = np.sum((X_train-Rpca)**2)/Rpca.shape[0]/Rpca.shape[1]\n",
    "print('PCA reconstruction error with 2 PCs: ' + str(round(err,3)));\n",
    "print(max(Zpca[:,0]))\n",
    "print(min(Zpca[:,0]))\n",
    "print(max(Zpca[:,1]))\n",
    "print(min(Zpca[:,1]))\n",
    "\n",
    "print(np.argmax(Zpca[:,0]))\n",
    "print(np.argmax(Zpca[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training of a dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/20\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.6710 - acc: 0.5309 - val_loss: 0.6250 - val_acc: 0.5083\n",
      "Epoch 2/20\n",
      "1360/1360 [==============================] - 0s 241us/step - loss: 0.5842 - acc: 0.6206 - val_loss: 0.5755 - val_acc: 0.7625\n",
      "Epoch 3/20\n",
      "1360/1360 [==============================] - 0s 268us/step - loss: 0.4992 - acc: 0.7809 - val_loss: 0.3996 - val_acc: 0.7958\n",
      "Epoch 4/20\n",
      "1360/1360 [==============================] - 0s 275us/step - loss: 0.3872 - acc: 0.8044 - val_loss: 0.5051 - val_acc: 0.7708\n",
      "Epoch 5/20\n",
      "1360/1360 [==============================] - 0s 253us/step - loss: 0.3575 - acc: 0.8235 - val_loss: 0.4983 - val_acc: 0.7875\n",
      "Epoch 6/20\n",
      "1360/1360 [==============================] - 0s 254us/step - loss: 0.3461 - acc: 0.8324 - val_loss: 0.4909 - val_acc: 0.7792\n",
      "Epoch 7/20\n",
      "1360/1360 [==============================] - 0s 249us/step - loss: 0.3398 - acc: 0.8301 - val_loss: 0.4255 - val_acc: 0.8000\n",
      "Epoch 8/20\n",
      "1360/1360 [==============================] - 0s 259us/step - loss: 0.3199 - acc: 0.8368 - val_loss: 0.4581 - val_acc: 0.8208\n",
      "Epoch 9/20\n",
      "1360/1360 [==============================] - 0s 239us/step - loss: 0.3119 - acc: 0.8449 - val_loss: 0.3527 - val_acc: 0.8208\n",
      "Epoch 10/20\n",
      "1360/1360 [==============================] - 0s 252us/step - loss: 0.3140 - acc: 0.8397 - val_loss: 0.5801 - val_acc: 0.7792\n",
      "Epoch 11/20\n",
      "1360/1360 [==============================] - 0s 284us/step - loss: 0.3535 - acc: 0.8243 - val_loss: 0.3199 - val_acc: 0.8333\n",
      "Epoch 12/20\n",
      "1360/1360 [==============================] - 0s 255us/step - loss: 0.2876 - acc: 0.8522 - val_loss: 0.3502 - val_acc: 0.7875\n",
      "Epoch 13/20\n",
      "1360/1360 [==============================] - 0s 239us/step - loss: 0.2696 - acc: 0.8596 - val_loss: 0.3318 - val_acc: 0.8333\n",
      "Epoch 14/20\n",
      "1360/1360 [==============================] - 0s 256us/step - loss: 0.2791 - acc: 0.8574 - val_loss: 0.3853 - val_acc: 0.7667\n",
      "Epoch 15/20\n",
      "1360/1360 [==============================] - 0s 267us/step - loss: 0.2734 - acc: 0.8632 - val_loss: 0.3521 - val_acc: 0.8125\n",
      "Epoch 16/20\n",
      "1360/1360 [==============================] - 0s 259us/step - loss: 0.2528 - acc: 0.8735 - val_loss: 0.3588 - val_acc: 0.8250\n",
      "Epoch 17/20\n",
      "1360/1360 [==============================] - 0s 307us/step - loss: 0.2797 - acc: 0.8706 - val_loss: 0.4267 - val_acc: 0.8042\n",
      "Epoch 18/20\n",
      "1360/1360 [==============================] - 0s 249us/step - loss: 0.2691 - acc: 0.8699 - val_loss: 0.3348 - val_acc: 0.8417\n",
      "Epoch 19/20\n",
      "1360/1360 [==============================] - 0s 251us/step - loss: 0.2362 - acc: 0.8809 - val_loss: 0.3932 - val_acc: 0.8208\n",
      "Epoch 20/20\n",
      "1360/1360 [==============================] - 0s 261us/step - loss: 0.2151 - acc: 0.8831 - val_loss: 0.4509 - val_acc: 0.8250\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(150,  activation='relu', input_shape=(105,)))\n",
    "#m.add(Dense(150,  activation='relu')) \n",
    "m.add(Dense(150,  activation='relu'))\n",
    "m.add(Dense(150,  activation='relu'))\n",
    "m.add(Dense(50,  activation='relu'))\n",
    "m.add(Dense(2,  activation='sigmoid'))\n",
    "m.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = m.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = m.fit(X_train, y_train, batch_size=300, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 66us/step\n",
      "Précision old features: 0.82\n"
     ]
    }
   ],
   "source": [
    "accuracy = m.evaluate(X_test, y_test)[1]\n",
    "print(\"Précision old features: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with ours now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('features_wave_julian_sarah.csv', delimiter=',')\n",
    "y = np.genfromtxt('output_wave_julian_sarah.csv', delimiter=',')\n",
    "\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "training_ratio = 0.85\n",
    "l = len(y)\n",
    "X_train = X[:int(l*training_ratio)]\n",
    "X_test = X[int(l*training_ratio):]\n",
    "y_train = y[:int(l*training_ratio)]/2\n",
    "y_test = y[int(l*training_ratio):]/2\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype(int))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we have 12 dimensions to features instead. Let's apply the NN directly without a PCA to compare later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/30\n",
      "1360/1360 [==============================] - 2s 1ms/step - loss: 0.6732 - acc: 0.5015 - val_loss: 0.6555 - val_acc: 0.5083\n",
      "Epoch 2/30\n",
      "1360/1360 [==============================] - 0s 122us/step - loss: 0.6434 - acc: 0.5000 - val_loss: 0.6294 - val_acc: 0.5250\n",
      "Epoch 3/30\n",
      "1360/1360 [==============================] - 0s 126us/step - loss: 0.6062 - acc: 0.5059 - val_loss: 0.5735 - val_acc: 0.5125\n",
      "Epoch 4/30\n",
      "1360/1360 [==============================] - 0s 126us/step - loss: 0.5548 - acc: 0.5346 - val_loss: 0.5615 - val_acc: 0.6167\n",
      "Epoch 5/30\n",
      "1360/1360 [==============================] - 0s 131us/step - loss: 0.5432 - acc: 0.5779 - val_loss: 0.5540 - val_acc: 0.5458\n",
      "Epoch 6/30\n",
      "1360/1360 [==============================] - 0s 131us/step - loss: 0.5438 - acc: 0.5375 - val_loss: 0.5550 - val_acc: 0.5292\n",
      "Epoch 7/30\n",
      "1360/1360 [==============================] - 0s 139us/step - loss: 0.5356 - acc: 0.5390 - val_loss: 0.5508 - val_acc: 0.5750\n",
      "Epoch 8/30\n",
      "1360/1360 [==============================] - 0s 133us/step - loss: 0.5386 - acc: 0.5787 - val_loss: 0.5510 - val_acc: 0.5792\n",
      "Epoch 9/30\n",
      "1360/1360 [==============================] - 0s 135us/step - loss: 0.5360 - acc: 0.6390 - val_loss: 0.5502 - val_acc: 0.7583\n",
      "Epoch 10/30\n",
      "1360/1360 [==============================] - 0s 131us/step - loss: 0.4548 - acc: 0.7743 - val_loss: 0.4399 - val_acc: 0.7708\n",
      "Epoch 11/30\n",
      "1360/1360 [==============================] - 0s 122us/step - loss: 0.4295 - acc: 0.7956 - val_loss: 0.3895 - val_acc: 0.8250\n",
      "Epoch 12/30\n",
      "1360/1360 [==============================] - 0s 137us/step - loss: 0.4167 - acc: 0.8029 - val_loss: 0.3943 - val_acc: 0.8042\n",
      "Epoch 13/30\n",
      "1360/1360 [==============================] - 0s 122us/step - loss: 0.4108 - acc: 0.7993 - val_loss: 0.3847 - val_acc: 0.7917\n",
      "Epoch 14/30\n",
      "1360/1360 [==============================] - 0s 136us/step - loss: 0.4138 - acc: 0.7985 - val_loss: 0.3707 - val_acc: 0.8000\n",
      "Epoch 15/30\n",
      "1360/1360 [==============================] - 0s 267us/step - loss: 0.4035 - acc: 0.8132 - val_loss: 0.3677 - val_acc: 0.7750\n",
      "Epoch 16/30\n",
      "1360/1360 [==============================] - 0s 162us/step - loss: 0.4008 - acc: 0.8103 - val_loss: 0.3643 - val_acc: 0.8083\n",
      "Epoch 17/30\n",
      "1360/1360 [==============================] - 0s 123us/step - loss: 0.3971 - acc: 0.8051 - val_loss: 0.3587 - val_acc: 0.8167\n",
      "Epoch 18/30\n",
      "1360/1360 [==============================] - 0s 128us/step - loss: 0.3957 - acc: 0.8096 - val_loss: 0.3634 - val_acc: 0.7667\n",
      "Epoch 19/30\n",
      "1360/1360 [==============================] - 0s 137us/step - loss: 0.3883 - acc: 0.8147 - val_loss: 0.3563 - val_acc: 0.7917\n",
      "Epoch 20/30\n",
      "1360/1360 [==============================] - 0s 167us/step - loss: 0.3866 - acc: 0.8140 - val_loss: 0.3489 - val_acc: 0.8250\n",
      "Epoch 21/30\n",
      "1360/1360 [==============================] - 0s 140us/step - loss: 0.3886 - acc: 0.8140 - val_loss: 0.3521 - val_acc: 0.8167\n",
      "Epoch 22/30\n",
      "1360/1360 [==============================] - 0s 137us/step - loss: 0.3782 - acc: 0.8250 - val_loss: 0.3456 - val_acc: 0.8333\n",
      "Epoch 23/30\n",
      "1360/1360 [==============================] - 0s 164us/step - loss: 0.3712 - acc: 0.8272 - val_loss: 0.3369 - val_acc: 0.8208\n",
      "Epoch 24/30\n",
      "1360/1360 [==============================] - 0s 131us/step - loss: 0.3734 - acc: 0.8228 - val_loss: 0.3305 - val_acc: 0.8208\n",
      "Epoch 25/30\n",
      "1360/1360 [==============================] - 0s 123us/step - loss: 0.3685 - acc: 0.8147 - val_loss: 0.3370 - val_acc: 0.8167\n",
      "Epoch 26/30\n",
      "1360/1360 [==============================] - 0s 122us/step - loss: 0.3643 - acc: 0.8228 - val_loss: 0.3209 - val_acc: 0.8208\n",
      "Epoch 27/30\n",
      "1360/1360 [==============================] - 0s 141us/step - loss: 0.3562 - acc: 0.8309 - val_loss: 0.3407 - val_acc: 0.8083\n",
      "Epoch 28/30\n",
      "1360/1360 [==============================] - 0s 129us/step - loss: 0.3510 - acc: 0.8390 - val_loss: 0.3295 - val_acc: 0.8167\n",
      "Epoch 29/30\n",
      "1360/1360 [==============================] - 0s 169us/step - loss: 0.3576 - acc: 0.8360 - val_loss: 0.3577 - val_acc: 0.8333\n",
      "Epoch 30/30\n",
      "1360/1360 [==============================] - 0s 122us/step - loss: 0.3631 - acc: 0.8316 - val_loss: 0.3089 - val_acc: 0.8625\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(15,  activation='relu', input_shape=(12,)))\n",
    "#m.add(Dense(15,  activation='relu')) \n",
    "m.add(Dense(15,  activation='relu'))\n",
    "m.add(Dense(15,  activation='relu'))\n",
    "m.add(Dense(4,  activation='relu'))\n",
    "m.add(Dense(2,  activation='sigmoid'))\n",
    "m.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = m.fit(X_train, y_train, batch_size=10, epochs=30, verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 0s 33us/step\n",
      "Précision New features: 0.86\n"
     ]
    }
   ],
   "source": [
    "accuracy = m.evaluate(X_test, y_test)[1]\n",
    "print(\"Précision New features: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We got a precision of 86%, which is better than the 82% of the last year. But we would need to do an average to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply PCA now to reduce the dimension to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca3 = PCA(n_components=3)\n",
    "\n",
    "# On entraîne notre modèle (fit) sur les données\n",
    "model_pca3.fit(X)\n",
    "\n",
    "# On applique le résultat sur nos données :\n",
    "X_reduced3 = model_pca3.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.85\n",
    "l = len(y)\n",
    "X_train = X_reduced3[:int(l*training_ratio)]\n",
    "X_test = X_reduced3[int(l*training_ratio):]\n",
    "y_train = y[:int(l*training_ratio)]/2\n",
    "y_test = y[int(l*training_ratio):]/2\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype(int))\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360 samples, validate on 240 samples\n",
      "Epoch 1/20\n",
      "1360/1360 [==============================] - 2s 2ms/step - loss: 0.5657 - acc: 0.7338 - val_loss: 0.5751 - val_acc: 0.8083\n",
      "Epoch 2/20\n",
      "1360/1360 [==============================] - 0s 139us/step - loss: 0.4888 - acc: 0.7757 - val_loss: 0.5545 - val_acc: 0.8125\n",
      "Epoch 3/20\n",
      "1360/1360 [==============================] - 0s 136us/step - loss: 0.4778 - acc: 0.7809 - val_loss: 0.5340 - val_acc: 0.8125\n",
      "Epoch 4/20\n",
      "1360/1360 [==============================] - 0s 142us/step - loss: 0.4688 - acc: 0.7868 - val_loss: 0.5265 - val_acc: 0.8083\n",
      "Epoch 5/20\n",
      "1360/1360 [==============================] - 0s 134us/step - loss: 0.4692 - acc: 0.7787 - val_loss: 0.5411 - val_acc: 0.8125\n",
      "Epoch 6/20\n",
      "1360/1360 [==============================] - 0s 133us/step - loss: 0.4655 - acc: 0.7875 - val_loss: 0.5312 - val_acc: 0.8167\n",
      "Epoch 7/20\n",
      "1360/1360 [==============================] - 0s 131us/step - loss: 0.4643 - acc: 0.7794 - val_loss: 0.5243 - val_acc: 0.8083\n",
      "Epoch 8/20\n",
      "1360/1360 [==============================] - 0s 133us/step - loss: 0.4624 - acc: 0.7890 - val_loss: 0.5306 - val_acc: 0.8125\n",
      "Epoch 9/20\n",
      "1360/1360 [==============================] - 0s 175us/step - loss: 0.4592 - acc: 0.7875 - val_loss: 0.5207 - val_acc: 0.8083\n",
      "Epoch 10/20\n",
      "1360/1360 [==============================] - 0s 143us/step - loss: 0.4551 - acc: 0.7838 - val_loss: 0.5152 - val_acc: 0.8083\n",
      "Epoch 11/20\n",
      "1360/1360 [==============================] - 0s 131us/step - loss: 0.4607 - acc: 0.7846 - val_loss: 0.4713 - val_acc: 0.8083\n",
      "Epoch 12/20\n",
      "1360/1360 [==============================] - 0s 136us/step - loss: 0.4481 - acc: 0.7904 - val_loss: 0.5133 - val_acc: 0.8125\n",
      "Epoch 13/20\n",
      "1360/1360 [==============================] - 0s 134us/step - loss: 0.4474 - acc: 0.7904 - val_loss: 0.5132 - val_acc: 0.8083\n",
      "Epoch 14/20\n",
      "1360/1360 [==============================] - 0s 150us/step - loss: 0.4383 - acc: 0.7926 - val_loss: 0.5065 - val_acc: 0.8125\n",
      "Epoch 15/20\n",
      "1360/1360 [==============================] - 0s 215us/step - loss: 0.4343 - acc: 0.7897 - val_loss: 0.4997 - val_acc: 0.8125\n",
      "Epoch 16/20\n",
      "1360/1360 [==============================] - 0s 157us/step - loss: 0.4284 - acc: 0.7890 - val_loss: 0.4992 - val_acc: 0.8083\n",
      "Epoch 17/20\n",
      "1360/1360 [==============================] - 0s 298us/step - loss: 0.4195 - acc: 0.7860 - val_loss: 0.4984 - val_acc: 0.8083\n",
      "Epoch 18/20\n",
      "1360/1360 [==============================] - 0s 215us/step - loss: 0.4184 - acc: 0.7897 - val_loss: 0.4881 - val_acc: 0.8083\n",
      "Epoch 19/20\n",
      "1360/1360 [==============================] - 0s 232us/step - loss: 0.4071 - acc: 0.7934 - val_loss: 0.4920 - val_acc: 0.8125\n",
      "Epoch 20/20\n",
      "1360/1360 [==============================] - 0s 224us/step - loss: 0.4053 - acc: 0.7897 - val_loss: 0.4305 - val_acc: 0.8083\n"
     ]
    }
   ],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(20,  activation='relu', input_shape=(3,)))\n",
    "#m.add(Dense(20,  activation='relu')) \n",
    "m.add(Dense(20,  activation='relu'))\n",
    "m.add(Dense(20,  activation='relu'))\n",
    "m.add(Dense(5,  activation='relu'))\n",
    "m.add(Dense(2,  activation='sigmoid'))\n",
    "m.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics=['accuracy'])\n",
    "\n",
    "history = m.fit(X_train, y_train, batch_size=10, epochs=20, verbose=1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a PCA doesn't seems to improve the accuracy, but the accuracy seems to be more stable with respect to the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
